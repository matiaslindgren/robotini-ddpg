env_kwargs:
  fps_limit: 60
  laps_per_episode: 1
  max_env_steps_per_episode: 4000
  forward_range: [0.02, 0.15]
  turn_range: [-1.0, 1.0]

actor:
  learning_rate: 0.0001
  kwargs:
    input_fc_layer_params: [512, 256]
    lstm_size: [256]
    output_fc_layer_params: null

critic:
  learning_rate: 0.0001
  kwargs:
    observation_fc_layer_params: null
    action_fc_layer_params: null
    joint_fc_layer_params: [512, 256]
    lstm_size: [256]
    output_fc_layer_params: null

target_updates_per_epoch: 0.5

agent_type: td3
agent_kwargs:
  target_update_tau: 0.005
  gamma: 0.98
  exploration_noise_std: 0.05
  target_policy_noise: 0.1
  target_policy_noise_clip: 0.2
  summarize_grads_and_vars: true
  debug_summaries: false

# agent_type: ddpg
# agent_kwargs:
#   target_update_tau: 0.005
#   gamma: 0.99
#   ou_stddev: 0.05
#   ou_damping: 0.05
#   summarize_grads_and_vars: true
#   debug_summaries: false


max_num_epochs: 1000
num_explore_envs: 8
num_eval_envs: 1

train_batches_per_epoch: 2
collect_batches_per_epoch: 1
initial_collect_batches: 10
num_eval_episodes: 10
eval_interval: 100

train_sequence_length: 100
batch_size: 64
replay_buffer_max_length: 2_000_000
use_tf_functions: true

checkpoint_interval: 1000
max_checkpoints_to_keep: 100
